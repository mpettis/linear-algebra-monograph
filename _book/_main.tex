% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Linear Algebra: An Intuitionist Approach},
  pdfauthor={Matt Pettis},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{braket}
\usepackage{mathtools}
\usepackage{amsmath}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}

\title{Linear Algebra: An Intuitionist Approach}
\author{Matt Pettis}
\date{2021-12-31}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{preface}{%
\chapter{Preface}\label{preface}}

\begin{quote}
And you may ask yourself, ``What is that beautiful house?''

And you may ask yourself, ``Where does that highway go to?''

And you may ask yourself, ``Am I right? Am I wrong?''

And you may say to yourself, ``My God! What have I done?''

-- Talking Heads, ``Once in a Lifetime''
\end{quote}

\begin{quote}
What one fool can do, another can.

-- Ancient Simian Proverb

-- Sylvanus Thompson, ``Calculus Made Easy''
\end{quote}

\begin{quote}
``Everything is the way it is because it got that way.''

-- D'Arcy Wentworth Thompson
\end{quote}

I'd like to say I'm writing this ``for the democratization of science and math,'' but really, for my kids and their friends so that they don't get snookered into thinking this stuff is beyond them and therefore not for them. It is for you. It is everybody's birthright.

One of the best things about science, but one I've found least talked about in the classes that I took in high school and college, is the part that explains ``why do we think things work this way?'' Why do we believe things are made up of atoms? Why did people believe that without the ability to \emph{see} atoms? What is it about the technology we've built that confirms that things are made of atoms? We believe things like this because we concocted hypotheses and made experimental tests that ruthlessly and cumulatively. We make assumptions that are verifiably true, and then we reach a little further with logic and some more subtle observations, and extend the things we get to conclude, and what we have to throw away. That is a powerful process.

Somewhere along the way, math got divorced from that process. It wasn't helped by great mathematicians like Carl Gauss who called Number Theory ``the Queen of Mathematics'' mostly because it didn't have much in the way of application, and that was a good thing. Or the eminent mathematician G. H. Hardy, who said,

\begin{quote}
``I have never done anything''useful``. No discovery of mine has made, or is likely to make, directly or indirectly, for good or ill, the least difference to the amenity of the world.''
\end{quote}

The perspective was, and often is, that math is a thing more akin to art, like poetry, and though it is sometimes useful, its main value is in that it is beautiful and fun. Ironically, his favorite subject, number theory, is the foundation of our ability to transmit secrets safely on the internet, and does, in fact, probably cause more good and ill than he was comfortable with.

The downside of such a perspective is that it makes it seem like learning the discipline of mathematics is inscrutable. When you encounter mathematical definitions, such as, ``What makes a thing a vector space?'', or ``What makes a thing a group?'', or ``What makes a set measureable?'', what you read are a bunch of seemingly awkward little statements that seem either indecipherable, or unknowable, or so stupidly dead-simple as to make you wonder why one would even need to say such a thing. For instance, when we get to the definition of a vector space, you'll see this as a defining characteristic that your, uh, we'll call a thingy for now, needs to have to be called a vector space:

\[ (\vec{x} + \vec{y}) + \vec{z} = \vec{x} + (\vec{y} + \vec{z}) \]

For those familiar with how numbers work, this seems like something Captain Obvious would say about math. It could also make you wonder ``what's the point of saying such a thing?''

These definitions don't come in an inspiration, like Athena springing fully formed from the forehead of Zeus. When you study the history of mathematics, you'll see that when trying to come up with descriptions like this, mathematicians will often argue, and even disagree violently. You'll often see different characterizations like this depending on where you look, because originators disagreed on what was \emph{fundamental} about what was going on. For systems to be compatible, though, the fundamental assumptions of one camp need to be at least derivable from the other, and vice versa.

This monograph is intended give you the motivations of why linear algebra is the way it is. It will address:

\begin{itemize}
\tightlist
\item
  What would make you come up with an idea like an inner product?
\item
  What's helpful about orthogonality?
\item
  Eigenvectors: how do they even work?
\end{itemize}

I'll approach this as science would ideally approach this. What do we observe? What sort of simplifications can we make to help us understand what is going on? What sort of models help us understand the important parts?

\hypertarget{a-nu-start}{%
\chapter{A Nu Start}\label{a-nu-start}}

It is probably intuitive that if you specify the lengths of two lines and the angles between them, then the tip-to-tip distance of the two lines is fixed, and can be calculated. Let's look at an example below. There, we assume that \(a\) and \(b\) are the two lines, and \(C\) is the angle between them. If we specify values for those three parts, then the length of \(c\) is also determined.

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth,height=0.75\textheight]{images/LofC-PythThm} 

}

\caption{Pythagorean Theorem, Law of Cosines}\label{fig:unnamed-chunk-1}
\end{figure}

If \(C\) is a right angle, like above, we can calculate \(c\) via the Pythagorean Theorem:

\[(\text{Pythagorean Theorem}) \ \ \ \ \ c^2 = a^2 + b^2\]
However, if \(C\) were not a right angle, but instead was any old angle, you could still compute the length of \(c\) if you knew the angles \(c\) makes with \(a\) and \(b\). It should be straightforward to see in the diagram that:

\[(1) \ \ \ \ \ c = b \ cos(A) + a \ cos(B)\]

With some fancy trigonometry, you can find a formula for \(c\) (or equally, \(c^2\)) with a formula that involves just the angle \(C\) instead of angles \(A\) and \(B\). We call that formula the \textbf{Law of Cosines}:

\[(\text{Law of Cosines}) \ \ \ \ \ c^2 = a^2 + b^2 - 2 a b \ cos(C)\]

You can see that it looks a lot like the Pythagorean Theorem. In fact, when \(C\) is 90 degrees, the last term is \(0\), and we are back to the Pythagorean Theorem.

That was a whirlwind tour of some nice formulas, and I didn't even annoy you with a proof of the Law of Cosines. It is nothing special, and just involves some clever trigonometric substitutions and identities using the sum of angles formulas, available anywhere you can search the internet.

One thing I'd like to highlight here is that the relationships that are helpful are posed as quadratics. That is, the Pythagorean Theorem relates the squares of the sides. If you were to talk in units, these relationships are in terms of \(length^2\), not straight \(length\). That quadratic relationship makes some things a bit harder, but other, more important things, a bit easier.

What I really want to highlight are some aspects of this problem that turn out to be the ones we need to hold on to when we want to develop the heavier machinery of linear algebra, where in many cases we can't always run back to a nice geometric picture to look at.

\hypertarget{lines-that-run-roughly-in-the-same-direction}{%
\section{Lines that run roughly in the same direction}\label{lines-that-run-roughly-in-the-same-direction}}

Let's go back to the triangle at the top of the page. Look at \(b\ cos(A)\), and imagine if you pull the point \(C\) straight up, following the path the perpendicular dotted line would go if it kept going up. What happens? Well, angle \(C\) gets smaller, and sides \(b\) and \(a\) get longer and longer. But the part of \(c\) made up by \(b\ cos(A)\) keeps staying the same length.

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth,height=0.75\textheight]{images/Triangles-bcosA} 

}

\caption{Triangles with same third side length}\label{fig:unnamed-chunk-2}
\end{figure}

In fact, if you make any triangle with the same horizontal line, and a vertex somewhere on the dashed line, then \(b\ cos(A)\) will be the same for any triangle you make in this manner.

Another way to think of this is: how much of \(b\) accounts for the total length of \(AC\)? In all of the triangles shown, the answer is: equal amounts. As the vertex gets further away from \(AC\), \(b\) gets \emph{longer}, but points in a direction that is more away from \(AC\) than in the same direction of it.

This idea -- the interplay of line lengths and how much they do or do not point in the same direction -- is at the heart of \emph{inner products}. Inner products can be used in many different disguises in different situations. But here, in normal-geometry Euclidian space, the inner product, also known as the dot product in this space, of two lines is the product of the length of those two lines and the cosine of the angle between them. That ``cosine of the angle between them'' accounts for how much in the same direction the two lines are pointing.

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth,height=0.75\textheight]{images/Triangles-bcosA-rightTriangle} 

}

\caption{Zero inner product of b and c}\label{fig:unnamed-chunk-3}
\end{figure}

One more important observation\ldots{} In the above, when \(A\) is a right angle, there is no \(b\ cos(C)\) component, or really, it is 0. Because \(b\) and \(c\) are pointing perpendicular to each other. In other words, \(b\) doesn't point at all in any direction of \(c\), so the inner (dot) product is zero. This is a condition that we will heavily exploit in all of linear algebra. You can get a hint as to why\ldots{} in this case, we get a right triangle, and the relation of the sides is given by the Pythagorean theorem, which has simpler terms than the Law of Cosines. This ``zero inner product'' condition will be exploited to make computations relatively straight-forward, and interpretations will be simplified as well.

\hypertarget{inner-products}{%
\section{Inner Products}\label{inner-products}}

Let's look at that adjustment factor to the Pythagorean Theorem that allows us to calculate any third side knowing the other two. Taking out the factor of \(-2\), we are left with:

\[(2) \ \ \ \ \ a b \ cos(C)\]
That's the inner product of the vectors \(\vec{a}\) and \(\vec{b}\), with \(C\) being the angle between those two vectors.

There's reason we go through all of these gymnastics, to come up with the Pythagorean Theorem and the Law of Cosines, and that is this: it will simplify some other really hard things.

Let's talk about vector spaces. I'll make assumptions at this point that you know what this is, in at least a geometric sense. It's the Euclidian space where you draw arrows, and you can add two arrows graphically by placing the tail of one vector on the head (arrow) part of the other, and drawing a new arrow from the base of the one vector to the tip of the other one. And you can ``scalar multiply'' an arrow by multiplying it by a real number, and that real number will make a new arrow along the same direction as the original arrow, but whose length is changed by the magnitude of the scalar (real number). Negative numbers will reverse the direction of the arrow, but the length is still changed by the magnitude of the scalar.

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth,height=0.75\textheight]{images/parallelogram-decomposition} 

}

\caption{Vector addition, scalar multiplication}\label{fig:unnamed-chunk-4}
\end{figure}

In the above picture, we have made the picture so that if you scale \(\vec{v1}\) by \(a1\) and \(\vec{v2}\) by \(a2\), and add them together, you get \(\vec{r}\). Or:

\[\vec{r} = a1\cdot\vec{v1} + a2\cdot\vec{v2}\]

Spoiler alert: inner products are the thing you need to compute to figure out the values for \(a1\) and \(a2\).

Here's the rub: vectors in a vector space are pretty much an unmanageable mess unless you put some effort into organizing them. The easiest way to make an accounting system is to pick a certain number of special fixed vectors and \emph{write every other vector in terms of those special vectors.}

For Euclidian space, the number of vectors you pick depends on the \emph{dimension} of your space. So, \(\mathbb{R}^2\), or the Cartesian Plane, requires that you pick only two vectors (and those vectors can be any two vectors as long as one isn't a scalar multiple of the other). \(\mathbb{R}^3\) takes 3 vectors (and in that set of 3, the restriction is that you can't write one of those vectors as a sum of scalar multiples of the other two). And so on.

Once you have your set of special vectors that meet the above criteria (this set is known as a basis), you can write any other vector as a sum of scalar multiple of those vectors. For example, if you've picked two vectors \(\vec{v1}\) and \(\vec{v2}\) from \(\mathbb{R}^2\), with the restriction above, you can write any other vector \(r\) in terms of them. To repeat our formula with slightly different terms:

\[\vec{r} = a1\cdot\vec{v1} + a2\cdot\vec{v2}\]
And for any \(r\), you can find scalars \(a1\) and \(a2\) that will make this work. The key is: \emph{how do you find \(a1\) and \(a2\)?}.

The answer is: inner products. Without going through the grungy math, I will give you the grungy result, which I will say \textbf{Don't scrutinize this too much.} I just want to make the point that the scalars you need can be calculated in terms of inner products and simple addition, subtraction, multiplication, and division.

\[a1 = \frac{(\vec{v1}\cdot\vec{v2})\cdot(\vec{v2}\cdot\vec{r})-(\vec{v2}\cdot\vec{v2})\cdot(\vec{v1}\cdot\vec{r})}{(\vec{v1}\cdot\vec{v2})^2 - (\vec{v1}\cdot\vec{v1})\cdot(\vec{v2}\cdot\vec{v2})}\]

\[a2 = \frac{(\vec{v1}\cdot\vec{v2})\cdot(\vec{v1}\cdot\vec{r})-(\vec{v1}\cdot\vec{v1})\cdot(\vec{v2}\cdot\vec{r})}{(\vec{v1}\cdot\vec{v2})^2 - (\vec{v1}\cdot\vec{v1})\cdot(\vec{v2}\cdot\vec{v2})}\]

Not the prettiest thing, but computable! And that is the goal. It may seem harder, but know that it is indeed far easier to pick a set of vectors and do these computations to represent them than it is to not have that.

The idea is: basic arithmetic and inner producst are cheap/easy to compute. Keeping track of scalars is also cheap/easy. Keeping track of individual vectors is hard. The easiest compromise is to use the fewest individual vectors, and as many scalars and vector additions as needed to decompose any given vector into a set of pre-picked vectors. That's what a \emph{basis} is: a pre-picked set of vectors with which we can express any other vector we want.

\hypertarget{orthogonality-and-normality}{%
\section{Orthogonality and Normality}\label{orthogonality-and-normality}}

Those calculations are indeed a bit gross, so people have looked for ways to make that not so gross. Above we noted that when the angle between vectors is 90 degrees, the cosine is 0. That means that if the angle between \(\vec{v1}\) and \(\vec{v2}\) is 90 degrees, then \(\vec{v1}\cdot\vec{v2} = 0\). If you, at the outset, \emph{pick} the vectors \(\vec{v1}\) and \(\vec{v2}\) to be ones where the angle between them is 90 degrees, then the above calculations simplify greatly to:

\[a1 = \frac{\vec{v1}\cdot\vec{r}}{\vec{v1}\cdot\vec{v1}}\]

\[a2 = \frac{\vec{v2}\cdot\vec{r}}{\vec{v2}\cdot\vec{v2}}\]
With this simplification, you should think, ``Wow, I should probably choose my vectors to be perpendicular to each other!'' And that is what orthogonality is -- vectors whose inner products are 0. When they are geometric vectors, like above, orthogonality is a synonym for perpendicularity. We just use \emph{orthogonality} in the future when we have inner product, but not good geometry to go with them.

To go one step further, as the dot product of a vector with itself is the square of the length of that vector, you can get even simpler expressions above if the length of \(\vec{v1}\) and \(\vec{v2}\) are \(= 1\). Then you can forget about the denominators, as you then get:

\[a1 = \vec{v1}\cdot\vec{r}\]
\[a2 = \vec{v2}\cdot\vec{r}\]
Having length one also has a general name, called being \emph{normal}, or \emph{normality}. And it too helps simplify calculations, like above. You can always compute a vector that is normal and in the same direction as your original vector if you divide the vector by its length. Or, scalar multiply it by its reciprocal length. So, for any vector \(\vec{v}\), regardless of length (other than 0), the following vector will have length 1:
\[\frac{1}{\sqrt{\vec{v}\cdot\vec{v}}}\cdot\vec{v}\]
Again -- same direction as \(\vec{v}\), but of length 1.

\hypertarget{gram-schmidt-orthonormalization}{%
\subsection{Gram-Schmidt Orthonormalization}\label{gram-schmidt-orthonormalization}}

Big words, straightforward idea.

TL;DR: Given a set of vectors that form a basis, but are not necessarily orthogonal to each other, or normal, You can use those vectors to create another set of vectors that will work as a basis, \emph{and} those vectors will be orthonormal.

Often, starting vectors for a given problem just kind of fall into your lap, and they are not orthogonal at the start. So how do we actually make an orthonormal set from one that isn't? Luckily, if you can use those starting vectors (that are not orthogonal) to make all of the other vectors you want, like above, you can can calculate a new set of vectors that will do the same thing, \emph{and} those vectors will be orthonormal. Here is a picture -- note the algebra looks worse than it is.

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth,height=0.75\textheight]{images/gram-schmidt} 

}

\caption{Gram-Schmidt: algebra looks worse than it is.}\label{fig:unnamed-chunk-5}
\end{figure}

Here's the idea, in straightforward language.

\begin{itemize}
\tightlist
\item
  Take your two vectors, \(\vec{a}\) and \(\vec{b}\), and pick one of them to start. Say, \(\vec{a}\).
\item
  Normalize it by dividing by it's length, and call the result \(v1\). Congrats, you have your first orthonormal vector of your new set.
\item
  Find the spot/vector along \(\vec{v1}\), which is the same line/direction as \(\vec{a}\), that will make a right triangle with \(\vec{b}\). Turns out, this calculation is straightforward, and is \(<\vec{b},\vec{v1}>\ \vec{v1}\), or \((\vec{b}\cdot\vec{v1})\ \vec{v1}\).
\item
  Subtract that from \(\vec{b}\), which is \(\vec{b} - (\vec{b}\cdot\vec{v1})\ \vec{v1}\). What you are left with is a vector perpendicular to \(\vec{v1}\), so you are almost home.
\item
  Divide that vector by its own length so that you have a normalized vector: \(\vec{v2} = \frac{1}{length(\vec{b} - (\vec{b}\cdot\vec{v1})\ \vec{v1})}\ (\vec{b} - (\vec{b}\cdot\vec{v1})\ \vec{v1})\). Not as ugly as it seems. Congrats, you now have an orthonormal basis with \(\vec{v1}\) and \(\vec{v2}\).
\end{itemize}

This process can be extended to more than 2 dimensions. It is a process of taking vectors, scaling them to length 1 for normalization, finding projections (making right triangles and picking the cosine leg), subtracting it from your next vector, and then normalizing again. It's like a great big geometric construction you might have done in geometry in high school with a compass and straight-edge. The process gets a little more algorithmically hairy for more dimensions, but it is repeating this same idea/process over and over again.

We will call the set of vectors that you can use to represent any other vector (with scalar multiplication and addition) as a \emph{basis}. If all of the vectors in that basis are of length 1 and are perpendicular, we call that an \emph{orthonormal basis}.

\hypertarget{summarizing-the-big-points}{%
\section{Summarizing the big points}\label{summarizing-the-big-points}}

We've only worked with \(\mathbb{R}^2\), but we've learned a lot and scaffolded ourselves to work with some more abstract ideas. They are:

\begin{itemize}
\tightlist
\item
  You can generalize the Pythagorean Theorem via the Law of Cosines. This allows you to compute the length of any leg of a triangle from the length of the other two sides and the angle between the other two sides.
\item
  The Law of Cosines has embedded in it a quantity we end up calling the \emph{inner product} of two vectors. This gives some measure of how much in the same direction two sides of a triangle point, and incorporate the length of those two sides so that ultimately you can get a Pythagorean Theorem like equation.
\item
  If we pick the right vectors, we can use the inner product, and standard addition/subtraction/multiplication/division to compute coefficients that can be used with those vectors to represent any other vector we want.
\item
  It's easier if the vectors we use to write other vectors with are mutually perpendicular and of length 1 (orthonormal).
\item
  We can always make an orthonormal set using the Gram-Schmidt process.
\end{itemize}

\hypertarget{in-progress---eigenvectors-and-eigenvalues}{%
\chapter{IN PROGRESS - Eigenvectors and Eigenvalues}\label{in-progress---eigenvectors-and-eigenvalues}}

Part of the motivation from the last chapter was to simplify calculations. The simplification process was something like this:

\begin{itemize}
\tightlist
\item
  Instead of talking about each vector uniquely, find a small set of vectors through which you can express every other vector in that space you could possibly want to talk about.
\item
  Be picky about that set, and pick the smallest set of vectors you can get away with.
\item
  Find a relatively easy function (inner product) that makes calculating the scalars that you need to use with that small set of vectors to represent any other vector.
\item
  Be more picky about that small set of vectors (the basis) in a way that makes computing the scalars more computationally simple -- that is by selecting a set of vectors that are mutually orthogonal. This makes a lot of terms in the scalar computation reduce to zero and make a simpler computation.
\item
  Be even more picky about that set and make sure the \emph{length} of the basis vectors are \(1\). This gives you an orthonormal basis.
\end{itemize}

These ways of being very picky about what basis set of vectors you choose to represent any other vector in the space pays off by making the scalar multiplier computations you need to make to get the right expression much more simple by introducing a lot of zero terms.

Having vectors lying around seems pretty limited in its usefulness. On their own, they don't really do much. You can think about using vectors to keep track of things like position of a particle, or velocity of an airplane, but by themselves, they don't give you much more than an accounting system of limited use.

So how do we make them useful? It turns out that one of the powerful things to do with vectors is to transform them from one vector into another. Why? One way is by giving you the tools to figure out where you have to aim an initial vector in order to hit a target vector. Let's do an example.

The common example is linear systems of equations. I grew up on a farm, so let's talk about Crazy Larry's Seed Shop. Here, you can use dollars or euros (because if I recall anything about my rural upbringing, it was very cosmopolitan). However, these are the rules for how Crazy Larry will sell you stuff (he's not crazy because of his low, low prices):

\begin{itemize}
\tightlist
\item
  For every dollar you pay him, he will give you 2 bags of corn and 2 bags of wheat.
\item
  For every euro you pay him, he will give you 3 bags of corn and 1 bag of wheat.
\item
  This is all proportional. Half a dollar will get you 1 bag of corn and 1 bag of wheat, etc.
\item
  If necessary, the trade can go the other way. Crazy Larry will pay you a dollar if you give him 2 bags of corn and 2 bags of wheat, etc.
\end{itemize}

That's the deal. Crazy, right?

So, you do some sample trades to get a feel for what goes on.

\begin{itemize}
\tightlist
\item
  If you give just a dollar or just a euro, you get what was laid out in the rules above -- very simple.
\item
  If you give 1 dollar and 1 euro, you get 2 bags of corn and 2 of wheat from the dollar, and 3 of corn and 1 of wheat from the euro, for a total of 5 bags of corn and 3 bags of wheat.
\item
  If you give 8 dollars and 3 euros, you get a total of 25 bags of corn and 19 bags of wheat.
\end{itemize}

Another thing that makes Crazy Larry crazy is that he is inflexible in these rules. You can't just ask him ``how much if I just want to buy corn alone?'' The rules are the rules, and you have to take what the rules buy you.

And so on. Its a lot of accounting. First, since we just spent a chapter on Euclidian vectors, let's model these calculations as vectors in a Euclidian space. Truthfully, that's probably the biggest advantage of Euclidian spaces -- you can model a lot of problems with vectors in them.

Ok, now say you have a target -- you know how much corn and wheat you want. Lets say you want 13 bags of corn and 9 bags of wheat. Is it even possible to buy this from Crazy Larry? You sit down, do some trial and error, and find out, yes, you can do this if you pay 3.5 dollars and 2 euros.

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth,height=0.75\textheight]{images/CrazyCornWheat} 

}

\caption{Crazy Larry, of course, has drawn out his crazy scheme.}\label{fig:unnamed-chunk-6}
\end{figure}

Well, you can see that you can model your payment and your seed bags as vectors easily on a Cartesian (Euclidian) plane. Let's dissect this and make the ideas explicit.

\begin{itemize}
\tightlist
\item
  Your payment is a vector, with the 2 components being the dollars and the euros you pay.
\item
  Your seed received is a vector, with the 2 components being corn and wheat.
\item
  Crazy Larry's rules are the transformation.
\end{itemize}

\hypertarget{old---inner-products-mathbbr2-pythagorean-theoroem-and-law-of-cosines}{%
\chapter{\texorpdfstring{OLD - Inner Products: \(\mathbb{R}^2\), Pythagorean Theoroem, and Law of Cosines}{OLD - Inner Products: \textbackslash mathbb\{R\}\^{}2, Pythagorean Theoroem, and Law of Cosines}}\label{old---inner-products-mathbbr2-pythagorean-theoroem-and-law-of-cosines}}

\begin{quote}
It takes a very unusual mind to undertake the analysis of the obvious.

-- Alfred North Whitehead
\end{quote}

\begin{quote}
Point of view is worth 80 IQ points

-- Alan Kay
\end{quote}

Here we are going to lay the groundwork vector spaces, particularly the aspect of inner products. We'll start with the most common prototype vector space: \(\mathbb{R}\). This will relate the intuition of geometry and trigonometry to the algebraic machinery of inner products.

\hypertarget{mathbbr2-the-pythagorean-theorem-and-the-law-of-cosines}{%
\section{\texorpdfstring{\(\mathbb{R}^2\), the Pythagorean Theorem, and the Law of Cosines}{\textbackslash mathbb\{R\}\^{}2, the Pythagorean Theorem, and the Law of Cosines}}\label{mathbbr2-the-pythagorean-theorem-and-the-law-of-cosines}}

Let's start with what we know pretty solidly -- the Pythagorean Theorem. We know that, if we have a triangle \(\triangle ABC\), with \(C\) being a right angle, then:

\[(\text{Pythagorean Theorem}) \ \ \ \ \ c^2 = a^2 + b^2\]

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth,height=0.75\textheight]{images/LofC-PythThm} 

}

\caption{Pythagorean Theorem}\label{fig:unnamed-chunk-7}
\end{figure}

The thing to note here is that the relationship of the three sides in a right triangle has a very tidy little relationship according to the theorem. \(\vec{c}^2\) is totally determined by \(a\)'s and \(b\)'s value in a right triangle.

But what happens if we keep \(\vec{a}\) and \(\vec{b}\) the same length, but start to wiggle the right angle at the top, making it smaller or larger?

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth,height=0.75\textheight]{images/LofC-acute-obtuse} 

}

\caption{Acute and obtuse triangles}\label{fig:unnamed-chunk-8}
\end{figure}

You'll note that the length of \(\vec{c}\) will shrink or grow from what it is in a right triangle with the same leg lengths. Can we make an equation like the Pythagorean Theorem that accounts for this deviation from what you get if you had a right triangle? Turns out, yes you can, and that \emph{adjusted} formula is known as \textbf{The Law of Cosines}:

\[(\text{Law of Cosines}) \ \ \ \ \ c^2 = a^2 + b^2 - 2 a b \ cos(C)\]

This should make some rough sense as a formula. As C shrinks below 90 degrees, the endpoints of the legs of the triangle get closer to each other, so the length of \(c\) shrinks. Algebraically, \(cos(C)\) becomes slightly positive, and \(-2 a b \ cos(C)\) will ultimately reduce the value of the right side of the equation, and \(c^2\) will be a little smaller than what it would be if \(C\) were a right triangle. In the other direction, as C grows above 90 degrees, the base of the triangle spreads, and \(c\) gets longer. Algebraically, \(cos(C)\) becomes slightly negative, and so \(-2 a b \ cos(C)\) becomes slightly positive, and \(c^2\) the right hand side makes \(c^2\) a little larger, so length \(c\) is a little larger.

I won't derive it here, as it is easy to find proofs. We should ask why this formula is important. Note that if you look at the diagrams, you will notice that:

\[(1) \ \ \ \ \ c = b \ cos(A) + a \ cos(B)\]

in all cases. That's pretty easy to see. What the Law of Cosines does, through some trigonometric identity substitutions, is re-express that equation, going from the angles that \(\vec{c}\) makes with \(\vec{a}\) and \(\vec{b}\), to using the angle that \(\vec{a}\) and \(\vec{b}\) make with each other instead. So the Law of cosines expresses \(\vec{c}\) totally in terms of the lengths of \(\vec{a}\) and \(\vec{b}\) and the angle they make with each other, never relying on information about \(c\).

It turns out that that this concept, of expressing vectors in terms of other vectors, is crucial in linear algebra, and we'll build up some concepts which may seem weird or abstract at the beginning, but turns out to be super useful in general, as it helps us lift off of reasoning about vector spaces in Euclidian space to general vector spaces.

\hypertarget{the-inner-product-or-dot-product}{%
\section{The Inner Product, or Dot Product}\label{the-inner-product-or-dot-product}}

Let's look at that adjustment factor to the Pythagorean Theorem that allows us to calculate any third side knowing the other two. Taking out the factor of \(-2\), we are left with:

\[(2) \ \ \ \ \ a b \ cos(C)\]

That particular computation, in Euclidian Space, is known as the \textbf{inner product of \(\vec{a}\) and \(\vec{b}\)}. It shows up in many contexts, and in general, the inner product will have different notations:
\[\vec{a} \cdot \vec{b}\]

\[ <\vec{a}, \vec{b}>\]

\[ <\vec{a}|\vec{b}>\]

How do we interpret this? There are a few components that go into building this up.

First is \(cos(C)\), which is the angle between \(\vec{a}\) and \(\vec{b}\). The cosine can be thought of as a measure of how much two lines, or vectors, point in the same direction. A value of 1 means they point completely in the same direction, -1 means they point in opposite directions, and 0 means that they are perpendicular to each other. We discussed above with the Law of Cosines that ``point-in-the-same-direction'' measure is baked into the adjustment to the Pythagorean Theorem to calculate the length of a third side of a triangle given the other two.

Second, how do we interpret multiplying the cosine by the lengths of \(a\) and \(b\)? Here we will make a stunning insight. Let's look back at equation (1). If we multiply both sides of the equation by \(c\), we get:

\[c^2 = cb\ cos(A) + ca\ cos(B)\]
or,
\[c^2 = \vec{c}\cdot\vec{b} + \vec{c}\cdot\vec{a}\]
by the definition of inner product we saw above. So if you squint, this kinda sorta looks like the Pythagorean Theorem. Furthermore, You can think of \(c^2\) as the inner product of a vector with itself, as the angle between a vector and itself can reasonably be thought of as 0, and \(cos(0) = 1\). So we can further rewrite the above as:

\[\vec{c}\cdot\vec{c} = \vec{c}\cdot\vec{b} + \vec{c}\cdot\vec{a}\]
Again, squinting, that is starting to look a lot like the Pythagorean Theorem, at least on the surface.

So, for Euclidian space, I like to think of the inner product, or dot product, as something you can calculate that accounts for how much in the same direction two vectors point (the cosine part), and add factors that make it dimensionally correct so that you can write a Pythagorean Theorem-like formula relating the lengths of sides of any triangle.

It turns out that the inner product has a lot of super-useful properties, mostly due to the high degree of symmetry it has.

What do I mean by ``symmetry''? Notice in the Pythagorean Theorem we distinguish between the hypotenuse and its legs. The formula only works when the square of the hypotenuse is on one side of the equation, and the squares of the legs are on the other side.

But this is not true of the Law of Cosines. The formula works no matter what two sides of the triangle you pick to derive the length of the third side from. That is, these formulas are all valid, and they have the same ``form'':

\[c^2 = a^2 + b^2 - 2 a b \ cos(C)\]
\[a^2 = c^2 + b^2 - 2 c b \ cos(A)\]
\[b^2 = a^2 + c^2 - 2 a c \ cos(B)\]

or,

\[\vec{c}\cdot\vec{c} = \vec{c}\cdot\vec{b} + \vec{c}\cdot\vec{a}\]
\[\vec{a}\cdot\vec{a} = \vec{a}\cdot\vec{b} + \vec{a}\cdot\vec{c}\]
\[\vec{b}\cdot\vec{b} = \vec{b}\cdot\vec{a} + \vec{b}\cdot\vec{c}\]

That's what I mean by symmetry, and that the Law of Cosines has it, but the Pythagorean Theorem does not.

\hypertarget{writing-vectors-in-terms-of-other-vectors}{%
\section{Writing vectors in terms of other vectors}\label{writing-vectors-in-terms-of-other-vectors}}

Apart from the nice observation that we can calculate lengths of a third side soley from information about the other two sides, who cares? We do. It turns out that a great simplification in dealing with vectors is if, instead of dealing with each vector on its own, like \(\vec{c}\) in \(\mathbb{R}^2\), we pick just two vectors that point in somewhat different directions, like \(\vec{a}\) and \(\vec{b}\), and then stretch or shrink those vectors (that is, make them point in the same direction, but of different lengths from the original), add them, and then use the sum of those two vectors as a stand-in for whatever vector \(\vec{c}\) we want. In math terms, we want to find multiples \(\alpha\) of \(\vec{a}\) and \(\beta\) of \(\vec{b}\) such that:

\[\vec{c} = \alpha\vec{a} + \beta\vec{b}\]
It may seem weird, but it does turn out that writing every possible vector \(\vec{c}\) in terms of two other vectors you pick at the outset (scalar multiples of \(\vec{a}\) and \(\vec{b}\)) makes other calculations you want to much more manageable. In the end, if you have some experience, you will probably end up picking \(\vec{a}\) and \(\vec{b}\) as you standard coordinate axes in the \(x\) and \(y\) direction, also denoted as either \(\hat{i}\) and \(\hat{j}\), or once you get into more dimensions than two, \(\hat{e}_{1}\) and \(\hat{e}_{2}\). But that will come later.

How do you do this? It turns out that you can calculate \(\alpha\) and \(\beta\) for that representation above purely with inner products, and combining inner products with the standard addition, subtraction, multiplication, and division operations. It's not pretty -- in fact, it looks kind of gross, but it is ultimately calculable. I won't go into the derivations of how you get there, but it turns out that with enough algebra, you can show that you can calculate the following. Mind you, don't look at the equations here right now and try to figure out how the terms are being used -- just note that you can indeed calculate the coefficients \(\alpha\) and \(\beta\) with just inner products and basic operations on numbers (add/subtract/multiply/divide). So, squint when you look at these equations:

\[\alpha = \frac{(\vec{a}\cdot\vec{b})\cdot(\vec{b}\cdot\vec{c})-(\vec{b}\cdot\vec{b})\cdot(\vec{a}\cdot\vec{c})}{(\vec{a}\cdot\vec{b})^2 - (\vec{a}\cdot\vec{a})\cdot(\vec{b}\cdot\vec{b})}\]
\[\beta = \frac{(\vec{a}\cdot\vec{b})\cdot(\vec{a}\cdot\vec{c})-(\vec{a}\cdot\vec{a})\cdot(\vec{b}\cdot\vec{c})}{(\vec{a}\cdot\vec{b})^2 - (\vec{a}\cdot\vec{a})\cdot(\vec{b}\cdot\vec{b})}\]
As I said, not the prettiest thing, but computable! And that is the goal. It may seem harder, but know that it is indeed far easier to pick a set of vectors and do these computations to represent them than it is to not have that.

\hypertarget{orthogonality-and-normality-1}{%
\section{Orthogonality and Normality}\label{orthogonality-and-normality-1}}

Those calculations are indeed a bit gross, so people have looked for ways to make that not so gross. Above we noted that when the angle between vectors is 90 degrees, the cosine is 0. That means that if the angle between \(\vec{a}\) and \(\vec{b}\) is 90 degrees, then \(\vec{a}\cdot\vec{b} = 0\). If you, at the outset, \emph{pick} the vectors \(\vec{a}\) and \(\vec{b}\) to be ones where the angle between them is 90 degrees, then the above calculations simplify greatly to:

\[\alpha = \frac{\vec{a}\cdot\vec{c}}{\vec{a}\cdot\vec{a}}\]

\[\beta = \frac{\vec{b}\cdot\vec{c}}{\vec{b}\cdot\vec{b}}\]
With this simplification, you should think, ``Wow, I should probably choose my vectors to be perpendicular to each other!'' And that is what orthogonality is -- vectors whose inner products are 0. When they are geometric vectors, like above, orthogonality is a synonym for perpendicularity. We just use \emph{orthogonality} in the future when we have inner product, but not good geometry to go with them.

To go one step further, as the dot product of a vector with itself is the square of the length of that vector, you can get even simpler expressions above if the length of \(\vec{a}\) and \(\vec{b}\) are \(= 1\). Then you can forget about the denomiators, as you then get:

\[\alpha = \vec{a}\cdot\vec{c}\]
\[\beta = \vec{b}\cdot\vec{c}\]
Having length one also has a general name, called being \emph{normal}, or \emph{normality}. And it too helps simplify calculations, like above. You can always compute a vector that is normal and in the same direction as your original vector if you divide the vector by its length. Or, scalar multiply it by its reciprocal length. So, for any vector \(\vec{v}\), regardless of length (other than 0), the following vector will have length 1:
\[\frac{1}{\sqrt{\vec{v}\cdot\vec{v}}}\cdot\vec{v}\]
Again -- same direction as \(\vec{v}\), but of length 1.

\hypertarget{gram-schmidt-orthonormalization-1}{%
\subsection{Gram-Schmidt Orthonormalization}\label{gram-schmidt-orthonormalization-1}}

Big words, straightforward idea.

Often, starting vectors for a given problem just kind of fall into your lap, and they are not orthogonal at the start. So how do we actually make an orthonormal set from one that isn't? Luckily, if you can use those starting vectors (that are not orthogonal) to make all of the other vectors you want, like above, you can can calculate a new set of vectors that will do the same thing, \emph{and} those vectors will be orthonormal. Here is a picture -- note the algebra looks worse than it is.

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth,height=0.75\textheight]{images/gram-schmidt} 

}

\caption{Gram-Schmidt: algebra looks worse than it is.}\label{fig:unnamed-chunk-9}
\end{figure}

Here's the idea, in straightforward language.

\begin{itemize}
\tightlist
\item
  Take your two vectors, \(\vec{a}\) and \(\vec{b}\), and pick one of them to start. Say, \(\vec{a}\).
\item
  Normalize it by dividing by it's length, and call the result \(v1\). Congrats, you have your first orthonormal vector of your new set.
\item
  Find the spot/vector along \(\vec{v1}\), which is the same line/direction as \(\vec{a}\), that will make a right triangle with \(\vec{b}\). Turns out, this calculation is straightforward, and is \(<\vec{b},\vec{v1}>\ \vec{v1}\), or \((\vec{b}\cdot\vec{v1})\ \vec{v1}\).
\item
  Subtract that from \(\vec{b}\), which is \(\vec{b} - (\vec{b}\cdot\vec{v1})\ \vec{v1}\). What you are left with is a vector perpendicular to \(\vec{v1}\), so you are almost home.
\item
  Divide that vector by its own length so that you have a normalized vector: \(\vec{v2} = \frac{1}{length(\vec{b} - (\vec{b}\cdot\vec{v1})\ \vec{v1})}\ (\vec{b} - (\vec{b}\cdot\vec{v1})\ \vec{v1})\). Not as ugly as it seems. Congrats, you now have an orthonormal basis with \(\vec{v1}\) and \(\vec{v2}\).
\end{itemize}

This process can be extended to more than 2 dimensions. It is a process of taking vectors, scaling them to length 1 for normalization, finding projections (making right triangles and picking the cosine leg), subtracting it from your next vector, and then normalizing again. It's like a great big geometric construction you might have done in geometry in high school with a compass and straight-edge. The process gets a little more algorithmically hairy for more dimensions, but it is repeating this same idea/process over and over again.

We will call the set of vectors that you can use to represent any other vector (with scalar multiplication and addition) as a \emph{basis}. If all of the vectors in that basis are of length 1 and are perpendicular, we call that an \emph{orthonormal basis}.

\hypertarget{summarizing-the-big-points-1}{%
\section{Summarizing the big points}\label{summarizing-the-big-points-1}}

We've only worked with \(\mathbb{R}^2\), but we've learned a lot and scaffolded ourselves to work with some more abstract ideas. They are:

\begin{itemize}
\tightlist
\item
  You can generalize the Pythagorean Theorem via the Law of Cosines. This allows you to compute the length of any leg of a triangle from the length of the other two sides and the angle between the other two sides.
\item
  The Law of Cosines has embedded in it a quantity we end up calling the \emph{inner product} of two vectors. This gives some measure of how much in the same direction two sides of a triangle point, and incorporate the length of those two sides so that ultimately you can get a Pythagorean Theorem like equation.
\item
  The inner product has a lot of symmetries that allow you to flexibly compute any of the sides.
\item
  If we pick the right vectors, we can use the inner product, and standard addition/subtraction/multiplication/division to compute coefficients that can be used with those vectors to represent any other vector we want.
\item
  It's easier if the vectors we use to write other vectors with are mutually perpendicular and of length 1 (orthonormal).
\item
  We can always make an orthonormal set using the Gram-Schmidt process.
\end{itemize}

\hypertarget{old---basic-intuitions}{%
\chapter{OLD - Basic Intuitions}\label{old---basic-intuitions}}

In the previous chapter, we did a lot of work figuring out how to calculate the length of a side of a triangle using information about the other two sides of the triangle. We also observed that we could pick almost any two vectors in \(\mathbb{R}^2\) and represent any other given vector by shrinking or stretching those two vectors independently and then adding them together.

We haven't talked about why we care, or why it is important that we can decompose lengths and vectors like this. Let's work by analogy with long multiplication. If you ask me to multiply \(87 \times 396\), I can't tell you the answer directly. But via the multiplication algorithm, which gets you to the answer by single-digit multiplications, keeping track of place value, and adding up the parts, I can get to the answer.

If we can decompose any vector down into a linear combination of the same two vectors, and we can determine a calculation on a vector by a linear

Vector spaces are based on a similar idea. Most computations on vectors are kind of hard. Let's assume we have a function \(f\) we want to calculate on a vector \(\vec{c}\) -- \(f(\vec{c})\). Assume we can write \(\vec{c}\) as:

\[\vec{c} = \alpha\cdot\vec{a} + \beta\cdot\vec{b}\]
So then

\[f(\vec{c}) = f(\alpha\cdot\vec{a} + \beta\cdot\vec{b})\]
Let's further assume that, like with multiplication, you can break your computation down by doing the same computation on the constituent parts. Like so:
\[f(\vec{c}) = \alpha\cdot f(\vec{a}) + \beta\cdot f(\vec{b})\]

If

In the previous chapter, we related the geometry and trigonometry of triangles to inner products. The main take-away there should be that inner products between two lines (or rather, vectors, which are directed lines, and which I here assume you have a background in) measure how much they point in the same direction, and account for how long each vector is. It turns out that with the definition of the inner product, we can write more general form of the Pythagorean Theorem that can be used to calculate any side length of a triangle in terms of the other two, instead of just for right triangles. Furthermore, knowing how vector addition and scalar multiples of vectors work, we can, for \(\mathbb{R}^2\), pick just about any two vectors, and write any other third vector in terms of a sum of scalar multiples of those original two vectors.

The motivating assumptions here are:

\begin{itemize}
\tightlist
\item
  Inner products are pretty easy to calculate, as are addition/subtraction/multiplication/division of scalars.
\item
  When working with vectors, it is easiest when you can represent any random vector in terms of a set of pre-determined vectors. And it is even better if those vectors in that set are all orthonormal. In this case, it make the calculations even simpler.
\end{itemize}

\hypertarget{properties-of-inner-products}{%
\section{Properties of inner products}\label{properties-of-inner-products}}

One of the properties of the inner product is that it is distributive. We always here this, but we are never really told why this is important. Having a function, like

\hypertarget{old-incomplete---quantum-linear-mapping}{%
\chapter{OLD, INCOMPLETE - Quantum Linear Mapping}\label{old-incomplete---quantum-linear-mapping}}

My uncle tells the story of how they help transition mechanical engineers fresh out of school to working in industry. A senior engineer tasks them with a simple task: a near complete machine needs a piece -- it is missing a cog. The tell the new engineer to get them one. The new engineer studies the plans, designs the cog, painstakingly laying out the CAD designs for it, and comes back to the senior engineer after a week with his work to review how to machine that part, and how much it will cost. The senior engineer looks it over, says it is the right piece, and pulls out a catalog, and shows him how much it will cost to buy, usually at a fraction of the cost. The lesson they learn: don't create from scratch stuff you can buy cheaply from a catalog.

We might think of physics and mathematics having a similar relationship. Often, as you painstakingly observe a physical system, you figure out what are the important bits you need to capture with a model, and how those bits behave in the system. Then, you go shopping to your local math department, describing what you are doing, and hope that a mathematician can recommend a system they've already investigated that you can map your problem into, and take advantage of the notions, the notations, and theorems about the system behavior that they've already come up with.

So, let's proceed by looking at a simple spin-\(1/2\) system, of electrons that can have either a spin up or down measurement, depending on how the spin-detector is oriented. Many newer books start their quantum discussion this way, and two I can recommend that do this are \emph{Quantum Mechanics: The Theoretical Minimum} by Len Susskind and \emph{Quantum Mechanics: A Paradigms Approach} by David McIntyre. I won't repeat their treatment here, and will assume you know the basics of measuring spins in those systems. I will talk about how you start to pick your mathematics from the catalog to model this phenomenon.

\hypertarget{lay-of-the-land}{%
\section{Lay of the Land}\label{lay-of-the-land}}

Here is a picture that captures some of the important information about a quantum experiment we run:

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth,height=0.75\textheight]{images/McIntyre_two-spin-experiment} 

}

\caption{Two-spin experiment, McIntire}\label{fig:unnamed-chunk-17}
\end{figure}

Some notes on the picture:

\begin{itemize}
\tightlist
\item
  The apparatus on the left emits electrons, which have a spin we want to measure.
\item
  The detectors are labeled with a `Z' or `X' to tell you which direction in space we are orienting the detector. We pick the directions arbitrarily, but `Z' and `X' are orthogonal directions.
\item
  The arrows on the detectors indicate which spin orientation an electron comes out of. If it is measured with a spin up, relative to the how the detector is oriented, it exits from the top port, with the up arrow. If down, it exits from the lower port.
\item
  The labels \texttt{\textbar{}+\textgreater{}} and \texttt{\textbar{}-\textgreater{}} are also labels for the spins. The indicate absolute directions of the spin (spin up or down in the Z direction), rather than the up and down arrows on the port, that indicate if the spin is up or down \emph{relative to the how the port is oriented}, in the `Z' or `X' direction.
\item
  The labels \texttt{\textbar{}+\textgreater{}}\_x and \texttt{\textbar{}-\textgreater{}}\_x represent the absolute spin up/down orientations, but for the X direction. Again, the detector can be reoriented, but if the electron has one of these labels, it is independent of how the detector is oriented.
\item
  The numbers and shaded bars represent a percentage of electrons that end up in that bucket or state over a large number of electrons entering the system. Like a histogram.
\end{itemize}

Here are some things you, the experimenter, observe about the system:

\begin{itemize}
\tightlist
\item
  In (a), if you measure the Z spin, then the X spin, then the second time you measure the Z spin, it will be 50-50 up/down.
\item
  In (b), it just tells you that the same thing happens, no matter if the secon measurement for X is up or down, like in experiment (a).
\item
  In (c), somthing really weird happens: If you put the X spin detector in the middle, but carefully recombine both beams, as if you didn't measure the X spin, then the Z spin will be as if you didn't measure X spin at all, and stays in the spin state you measured it in in the first detector.
\end{itemize}

That can stand as the first of many weirdnesses you encounter in quantum. As McIntyre says: going from (a) or (b) to (c), it's as if you are in a half-lit room, throw open a window shade, and the whole room goes dark. In a classical model, (c) would still have a 50-50 split of spin measurements, but that doesn't happen in the quantum world.

\hypertarget{what-things-do-we-need-to-model-this}{%
\section{What things do we need to model this?}\label{what-things-do-we-need-to-model-this}}

So, we make the following observations.

\begin{itemize}
\tightlist
\item
  If we measure a spin as up in the Z direction, and keep measuring the spin with detectors all pointed along the same Z axis, we will always get the same spin up measurement. That is true as long as we don't orient the detector in a different direction.
\item
  Focusing on Z measurements, we have two states: spin up (\texttt{\textbar{}+\textgreater{}}) and spin down (\texttt{\textbar{}-\textgreater{}}). We get that with the detector registering a +1 or -1.
\item
  If we measure a Z spin with a detector, and it registers a +1, we call it spin up, and take a second, identical detector, and flip it upside, it will register a -1. This scenario is not pictured.
\item
  If we take a Z detector and measure a spin up electron, and then take a second detector, and start to slightly tilt the detector away from a straight Z orientation, we still only measure +1 and -1 readings. For a slight tilt, almost all electrons will measure spin up, and a few spin down. As it rotates to be perpendicular to the Z direction, electrons measure +1 and -1 with a 50\% probability. As we get closer to the tilt making the detector upside down, then most of the electrons will measure -1, until it is exactly upside down, and the detetcor will consistently measure -1.
\end{itemize}

This is kind of weird. Note that if this were a classical spin, you would expect that if you measured a spin of +1, and tilted the detector a little bit, you would measure a value a little less than +1. But quantum doesn't work that way. Instead of reducing the spin a little bit, what happens is that the probability of a +1 starts to decrease as you tilt the detector. The \emph{average} of multiple measurements approaches the value of what you would expect a single measured spin to decrease by (if it were classical).

So, whatever math we come up, it can't be the same classical math that gives a single spin measurement decreasing continuously from +1. It has to be something that accounts for:

\begin{itemize}
\tightlist
\item
  A spin will always either be +1 or -1
\item
  The \emph{probability} of detecting +1 and -1 change as you tilt the detector.
\end{itemize}

So we go talk to the math department\ldots{}

  \bibliography{book.bib,packages.bib}

\end{document}
