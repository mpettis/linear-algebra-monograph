# OLD - Inner Products: $\mathbb{R}^2$, Pythagorean Theoroem, and Law of Cosines

> It takes a very unusual mind to undertake the analysis of the obvious.
>
> -- Alfred North Whitehead

> Point of view is worth 80 IQ points
>
> -- Alan Kay

Here we are going to lay the groundwork vector spaces, particularly the aspect of inner products.  We'll start with the most common prototype vector space: $\mathbb{R}$.  This will relate the intuition of geometry and trigonometry to the algebraic machinery of inner products.


## $\mathbb{R}^2$, the Pythagorean Theorem, and the Law of Cosines

Let's start with what we know pretty solidly -- the Pythagorean Theorem.  We know that, if we have a triangle $\triangle ABC$, with $C$ being a right angle, then:

$$(\text{Pythagorean Theorem}) \ \ \ \ \ c^2 = a^2 + b^2$$

```{r, echo=FALSE, out.width="75%", out.height="75%", fig.align="center", fig.cap="Pythagorean Theorem"}
knitr::include_graphics('images/LofC-PythThm.jpg')
```

The thing to note here is that the relationship of the three sides in a right triangle has a very tidy little relationship according to the theorem.  $\vec{c}^2$ is totally determined by $a$'s and $b$'s value in a right triangle.

But what happens if we keep $\vec{a}$ and $\vec{b}$ the same length, but start to wiggle the right angle at the top, making it smaller or larger?

```{r, echo=FALSE, out.width="75%", out.height="75%", fig.align="center", fig.cap="Acute and obtuse triangles"}
knitr::include_graphics('images/LofC-acute-obtuse.jpg')
```

You'll note that the length of $\vec{c}$ will shrink or grow from what it is in a right triangle with the same leg lengths.  Can we make an equation like the Pythagorean Theorem that accounts for this deviation from what you get if you had a right triangle?  Turns out, yes you can, and that *adjusted* formula is known as **The Law of Cosines**:

$$(\text{Law of Cosines}) \ \ \ \ \ c^2 = a^2 + b^2 - 2 a b \ cos(C)$$

This should make some rough sense as a formula.  As C shrinks below 90 degrees, the endpoints of the legs of the triangle get closer to each other, so the length of $c$ shrinks.  Algebraically, $cos(C)$ becomes slightly positive, and $-2 a b \ cos(C)$ will ultimately reduce the value of the right side of the equation, and $c^2$ will be a little smaller than what it would be if $C$ were a right triangle.  In the other direction, as C grows above 90 degrees, the base of the triangle spreads, and $c$ gets longer.  Algebraically, $cos(C)$ becomes slightly negative, and so $-2 a b \ cos(C)$ becomes slightly positive, and $c^2$ the right hand side makes $c^2$ a little larger, so length $c$ is a little larger.

I won't derive it here, as it is easy to find proofs.  We should ask why this formula is important. Note that if you look at the diagrams, you will notice that:

$$(1) \ \ \ \ \ c = b \ cos(A) + a \ cos(B)$$

in all cases.  That's pretty easy to see.  What the Law of Cosines does, through some trigonometric identity substitutions, is re-express that equation, going from the angles that $\vec{c}$ makes with $\vec{a}$ and $\vec{b}$, to using the angle that $\vec{a}$ and $\vec{b}$ make with each other instead.  So the Law of cosines expresses $\vec{c}$ totally in terms of the lengths of $\vec{a}$ and $\vec{b}$ and the angle they make with each other, never relying on information about $c$.

It turns out that that this concept, of expressing vectors in terms of other vectors, is crucial in linear algebra, and we'll build up some concepts which may seem weird or abstract at the beginning, but turns out to be super useful in general, as it helps us lift off of reasoning about vector spaces in Euclidian space to general vector spaces.



## The Inner Product, or Dot Product

Let's look at that adjustment factor to the Pythagorean Theorem that allows us to calculate any third side knowing the other two.  Taking out the factor of $-2$, we are left with:

$$(2) \ \ \ \ \ a b \ cos(C)$$

That particular computation, in Euclidian Space, is known as the **inner product of $\vec{a}$ and $\vec{b}$**.  It shows up in many contexts, and in general, the inner product will have different notations:
$$\vec{a} \cdot \vec{b}$$

$$ <\vec{a}, \vec{b}>$$

$$ <\vec{a}|\vec{b}>$$

How do we interpret this?  There are a few components that go into building this up.

First is $cos(C)$, which is the angle between $\vec{a}$ and $\vec{b}$.  The cosine can be thought of as a measure of how much two lines, or vectors, point in the same direction.  A value of 1 means they point completely in the same direction, -1 means they point in opposite directions, and 0 means that they are perpendicular to each other.  We discussed above with the Law of Cosines that "point-in-the-same-direction" measure is baked into the adjustment to the Pythagorean Theorem to calculate the length of a third side of a triangle given the other two.

Second, how do we interpret multiplying the cosine by the lengths of $a$ and $b$?  Here we will make a stunning insight.  Let's look back at equation (1).  If we multiply both sides of the equation by $c$, we get:

$$c^2 = cb\ cos(A) + ca\ cos(B)$$
or,
$$c^2 = \vec{c}\cdot\vec{b} + \vec{c}\cdot\vec{a}$$
by the definition of inner product we saw above.  So if you squint, this kinda sorta looks like the Pythagorean Theorem.  Furthermore, You can think of $c^2$ as the inner product of a vector with itself, as the angle between a vector and itself can reasonably be thought of as 0, and $cos(0) = 1$.  So we can further rewrite the above as:

$$\vec{c}\cdot\vec{c} = \vec{c}\cdot\vec{b} + \vec{c}\cdot\vec{a}$$
Again, squinting, that is starting to look a lot like the Pythagorean Theorem, at least on the surface.

So, for Euclidian space, I like to think of the inner product, or dot product, as something you can calculate that accounts for how much in the same direction two vectors point (the cosine part), and add factors that make it dimensionally correct so that you can write a Pythagorean Theorem-like formula relating the lengths of sides of any triangle.

It turns out that the inner product has a lot of super-useful properties, mostly due to the high degree of symmetry it has.

What do I mean by "symmetry"?  Notice in the Pythagorean Theorem we distinguish between the hypotenuse and its legs.  The formula only works when the square of the hypotenuse is on one side of the equation, and the squares of the legs are on the other side.

But this is not true of the Law of Cosines.  The formula works no matter what two sides of the triangle you pick to derive the length of the third side from.  That is, these formulas are all valid, and they have the same "form":

$$c^2 = a^2 + b^2 - 2 a b \ cos(C)$$
$$a^2 = c^2 + b^2 - 2 c b \ cos(A)$$
$$b^2 = a^2 + c^2 - 2 a c \ cos(B)$$

or,

$$\vec{c}\cdot\vec{c} = \vec{c}\cdot\vec{b} + \vec{c}\cdot\vec{a}$$
$$\vec{a}\cdot\vec{a} = \vec{a}\cdot\vec{b} + \vec{a}\cdot\vec{c}$$
$$\vec{b}\cdot\vec{b} = \vec{b}\cdot\vec{a} + \vec{b}\cdot\vec{c}$$


That's what I mean by symmetry, and that the Law of Cosines has it, but the Pythagorean Theorem does not.





## Writing vectors in terms of other vectors

Apart from the nice observation that we can calculate lengths of a third side soley from information about the other two sides, who cares?  We do.  It turns out that a great simplification in dealing with vectors is if, instead of dealing with each vector on its own, like $\vec{c}$ in $\mathbb{R}^2$, we pick just two vectors that point in somewhat different directions, like $\vec{a}$ and $\vec{b}$, and then stretch or shrink those vectors (that is, make them point in the same direction, but of different lengths from the original), add them, and then use the sum of those two vectors as a stand-in for whatever vector $\vec{c}$ we want.  In math terms, we want to find multiples $\alpha$ of $\vec{a}$ and $\beta$ of $\vec{b}$ such that:

$$\vec{c} = \alpha\vec{a} + \beta\vec{b}$$
It may seem weird, but it does turn out that writing every possible vector $\vec{c}$ in terms of two other vectors you pick at the outset (scalar multiples of $\vec{a}$ and $\vec{b}$) makes other calculations you want to much more manageable.  In the end, if you have some experience, you will probably end up picking $\vec{a}$ and $\vec{b}$ as you standard coordinate axes in the $x$ and $y$ direction, also denoted as either $\hat{i}$ and $\hat{j}$, or once you get into more dimensions than two, $\hat{e}_{1}$ and $\hat{e}_{2}$.  But that will come later.

How do you do this?  It turns out that you can calculate $\alpha$ and $\beta$ for that representation above purely with inner products, and combining inner products with the standard addition, subtraction, multiplication, and division operations.  It's not pretty -- in fact, it looks kind of gross, but it is ultimately calculable.  I won't go into the derivations of how you get there, but it turns out that with enough algebra, you can show that you can calculate the following.  Mind you, don't look at the equations here right now and try to figure out how the terms are being used -- just note that you can indeed calculate the coefficients $\alpha$ and $\beta$ with just inner products and basic operations on numbers (add/subtract/multiply/divide).  So, squint when you look at these equations:

$$\alpha = \frac{(\vec{a}\cdot\vec{b})\cdot(\vec{b}\cdot\vec{c})-(\vec{b}\cdot\vec{b})\cdot(\vec{a}\cdot\vec{c})}{(\vec{a}\cdot\vec{b})^2 - (\vec{a}\cdot\vec{a})\cdot(\vec{b}\cdot\vec{b})}$$
$$\beta = \frac{(\vec{a}\cdot\vec{b})\cdot(\vec{a}\cdot\vec{c})-(\vec{a}\cdot\vec{a})\cdot(\vec{b}\cdot\vec{c})}{(\vec{a}\cdot\vec{b})^2 - (\vec{a}\cdot\vec{a})\cdot(\vec{b}\cdot\vec{b})}$$
As I said, not the prettiest thing, but computable!  And that is the goal.  It may seem harder, but know that it is indeed far easier to pick a set of vectors and do these computations to represent them than it is to not have that.



## Orthogonality and Normality

Those calculations are indeed a bit gross, so people have looked for ways to make that not so gross.  Above we noted that when the angle between vectors is 90 degrees, the cosine is 0.  That means that if the angle between $\vec{a}$ and $\vec{b}$ is 90 degrees, then $\vec{a}\cdot\vec{b} = 0$.  If you, at the outset, *pick* the vectors $\vec{a}$ and $\vec{b}$ to be ones where the angle between them is 90 degrees, then the above calculations simplify greatly to:

$$\alpha = \frac{\vec{a}\cdot\vec{c}}{\vec{a}\cdot\vec{a}}$$

$$\beta = \frac{\vec{b}\cdot\vec{c}}{\vec{b}\cdot\vec{b}}$$
With this simplification, you should think, "Wow, I should probably choose my vectors to be perpendicular to each other!"  And that is what orthogonality is -- vectors whose inner products are 0.  When they are geometric vectors, like above, orthogonality is a synonym for perpendicularity.  We just use *orthogonality* in the future when we have inner product, but not good geometry to go with them.

To go one step further, as the dot product of a vector with itself is the square of the length of that vector, you can get even simpler expressions above if the length of $\vec{a}$ and $\vec{b}$ are $= 1$.  Then you can forget about the denomiators, as you then get:

$$\alpha = \vec{a}\cdot\vec{c}$$
$$\beta = \vec{b}\cdot\vec{c}$$
Having length one also has a general name, called being *normal*, or *normality*.  And it too helps simplify calculations, like above.  You can always compute a vector that is normal and in the same direction as your original vector if you divide the vector by its length.  Or, scalar multiply it by its reciprocal length.  So, for any vector $\vec{v}$, regardless of length (other than 0), the following vector will have length 1:
$$\frac{1}{\sqrt{\vec{v}\cdot\vec{v}}}\cdot\vec{v}$$
Again -- same direction as $\vec{v}$, but of length 1.



### Gram-Schmidt Orthonormalization

Big words, straightforward idea.

Often, starting vectors for a given problem just kind of fall into your lap, and they are not orthogonal at the start.  So how do we actually make an orthonormal set from one that isn't?  Luckily, if you can use those starting vectors (that are not orthogonal) to make all of the other vectors you want, like above, you can can calculate a new set of vectors that will do the same thing, *and* those vectors will be orthonormal.  Here is a picture -- note the algebra looks worse than it is.

```{r, echo=FALSE, out.width="75%", out.height="75%", fig.align="center", fig.cap="Gram-Schmidt: algebra looks worse than it is."}
knitr::include_graphics('images/gram-schmidt.jpg')
```

Here's the idea, in straightforward language.

* Take your two vectors, $\vec{a}$ and $\vec{b}$, and pick one of them to start.  Say, $\vec{a}$.
* Normalize it by dividing by it's length, and call the result $v1$.  Congrats, you have your first orthonormal vector of your new set.
* Find the spot/vector along $\vec{v1}$, which is the same line/direction as $\vec{a}$, that will make a right triangle with $\vec{b}$.  Turns out, this calculation is straightforward, and is $<\vec{b},\vec{v1}>\ \vec{v1}$, or $(\vec{b}\cdot\vec{v1})\ \vec{v1}$.
* Subtract that from $\vec{b}$, which is $\vec{b} - (\vec{b}\cdot\vec{v1})\ \vec{v1}$.  What you are left with is a vector perpendicular to $\vec{v1}$, so you are almost home.
* Divide that vector by its own length so that you have a normalized vector: $\vec{v2} = \frac{1}{length(\vec{b} - (\vec{b}\cdot\vec{v1})\ \vec{v1})}\ (\vec{b} - (\vec{b}\cdot\vec{v1})\ \vec{v1})$. Not as ugly as it seems.  Congrats, you now have an orthonormal basis with $\vec{v1}$ and $\vec{v2}$.

This process can be extended to more than 2 dimensions.  It is a process of taking vectors, scaling them to length 1 for normalization, finding projections (making right triangles and picking the cosine leg), subtracting it from your next vector, and then normalizing again.  It's like a great big geometric construction you might have done in geometry in high school with a compass and straight-edge.  The process gets a little more algorithmically hairy for more dimensions, but it is repeating this same idea/process over and over again.


We will call the set of vectors that you can use to represent any other vector (with scalar multiplication and addition) as a *basis*.  If all of the vectors in that basis are of length 1 and are perpendicular, we call that an *orthonormal basis*.


## Summarizing the big points

We've only worked with $\mathbb{R}^2$, but we've learned a lot and scaffolded ourselves to work with some more abstract ideas.  They are:

* You can generalize the Pythagorean Theorem via the Law of Cosines.  This allows you to compute the length of any leg of a triangle from the length of the other two sides and the angle between the other two sides.
* The Law of Cosines has embedded in it a quantity we end up calling the *inner product* of two vectors.  This gives some measure of how much in the same direction two sides of a triangle point, and incorporate the length of those two sides so that ultimately you can get a Pythagorean Theorem like equation.
* The inner product has a lot of symmetries that allow you to flexibly compute any of the sides.
* If we pick the right vectors, we can use the inner product, and standard addition/subtraction/multiplication/division to compute coefficients that can be used with those vectors to represent any other vector we want.
* It's easier if the vectors we use to write other vectors with are mutually perpendicular and of length 1 (orthonormal).
* We can always make an orthonormal set using the Gram-Schmidt process.




















<!--
# Intuition from $\mathbb{R}^2$

At this point, if you are reading this as opposed to an intro to linear algebra book, I assume the one thing you have is a good familiarity with are the vector spaces of $\mathbb{R}^2$ and $\mathbb{R}^3$.  These are the vector spaces used extensively in physics and engineering to model things like position, velocity, and forces.

You know that you can add two vectors by putting the tail of one vector at the head of the other and connecting the base of one to the tip of the other vector.

```{r, echo=FALSE, out.width="75%", out.height="75%", fig.align="center", fig.cap="Parallelogram Law of Addition"}
knitr::include_graphics('images/vector_parallelogram_law.png')
```

And we know how to stretch, shrink, and flip a vector, which we do by multiplying a vector by a real number.  If you multiply a vector by $1/2$, the length of the vector shrinks to one-half its original length (but the direction doesn't change).  If you multiply a vector by $3 \sqrt{2}$ the magnitude of the result is stretched by that amount.

```{r, echo=FALSE, out.width="100%", out.height="100%", fig.align="center", fig.cap="Vector Scaling"}
knitr::include_graphics('images/Vector-scaling.jpg')
```

These are the prototypical vector spaces, one I'd argue 99 times out of 100 people imagine if they know what a vector space is already.  And if this is the only space you have to deal with, this intuition is pretty much all you need.  It will take you far. But if you study quantum state space vectors, with their complex entries, and the notion of $\mathbb{R}^n$ vector spaces can help some of your intuition, and for other parts, you are totally at sea as to how the hell you are supposed to interpret things.

So it may help to go back to $\mathbb{R}^n$ and do some careful syntax-to-interpretation explanations.  And take some of the things we take for granted as obvious and cast them in new, abstract lights.

We just discussed how addition and scalar multiplication are performed.  But we should note the choices we made to do what seems so natural to us.

First, addition.  We did it using the parallelogram law.  How come we didn't choose to make the resulting vector have a length equal to the sum of the length of the two starting vectors?  You can see from the picture the length of our resulting vector is usually shorter, but never longer, than the length of the two vectors you are adding.  You should counter, "well, that's because things like forces don't work that way.  If I have two equal and opposite forces acting on an object, I want the result to be no net force.  If the magnitudes were added, which direction would the result point?  Doesn't make sense, so we can't use that because it doesn't model the situation."

Good answer, reader!

We want our model to reflect what is going on.  So that sort of addition is out.  It turns out head-to-tail addition does model things like forces.  What is also intuitive is that for any arrow, or force, or velocity, you can "counteract" it with another arrow/force/velocity.  You could not do this if the magnitues themselves always add.  You can never add two positive things to get a net zero, which would represent two things counteracting each other.

Second, just what do we mean by multiplying a vector by a scalar?  That's a little trickier.  We know what we mean: we shrink and grow the vector, in the same direction, by the scalar's sign and magnitude.  As above, multiplying a vector by $1/2$ shrinks its length to $1/2$ the original length, but the direction stays the same.

But if you are an engineer or scientist, your unit-analysis Spidey-sense should start to tingle... if scalars and vectors are different things, and I multiply them, whatever happens, I shouldn't get a vector out, like we are seeing.

A dirty little detail is usually glossed over.  Because it is short-circuiting to your intuition about stretching or shrinking, people don't talk about what $\alpha \vec{x}$ is.  Your brain probably assumes it is a multiplication, and what the scalar is multiplying is the "magnitude" property of the vector.  You can think about it that way for $\mathbb{R}^n$ vectors.  But that's not really a scalar-vector multiplication.  What it really is is the scalar acting as a function that maps a vector to another vector.  Like so:

$$ f_{\alpha} : V \to V , \alpha \in \mathbb{R} $$
So, $\alpha \vec{x}$ is really shorthand for $f_{\alpha}(\vec{x})$.

For $\mathbb{R}^n$, that function is the function that stretches or shrinks the magnitude of the vector by $\alpha$.  Again, this may seem like nit-picky pedantry, but we have to start leveraging this fact in the near future to keep some other things clear...


## The Axioms

So if vector spaces were just stretching and head-to-tail vector additions, mathematicians probably wouldn't make the seemingly awkward definition that are always presented in the first three pages of any textbook:


From Wikipedia:

```{r, echo=FALSE, out.width="100%", out.height="100%", fig.align="center", fig.cap="[Record scratch]  Yep, that's me, Vector Space, spewing a lot of incomprehensible stuff.  But I wasn't always like this.  Let me tell you a story..."}
knitr::include_graphics('images/vector-space-axioms-wikipedia_2.png')
```

It is a bad way to start, because you really have no frame of reference for what any of the terms mean.  It is frustrating and made me angry  **We shouldn't really start here.**

This reminds me of the joke about a cab driver driving around the Seattle area in a fog, and he asks a guy coming out of a building if he can tell the cabbie where he is.  The guy looks at him and says, "You're in a cab," and walks on.  The cabbie says "Perfect, I know where I am."  The fare asks him how he can figure out anything from what the guy outside said, and the cabbie say, "Well, he told me something that was completely true and completely useless.  So this must be the Microsoft building."

There's so many questions that should be triggered.  Like:

- Why so many simple axioms?
- What are vectors, really?
- What is a field, really?
- What does it mean to add vectors?
- What does it mean to multiply a vector by a scalar?
- How do I know what other vector $a \cdot \vec{u}$ becomes?
- It looks like the scalar field talks about how *scalar addition* and *scalar multiplication* works.  But vectors only talk about *vector addition*.  What's up with that?

With $\mathbb{R}^n$, we can look over the axioms and confirm that yep, they meet the axioms, because we have a strong intuition about how addtion of vectors work, how scalars stretch the vectors, and how the addition and multiplication of scalar (read: real) numbers work.

We can boil these axioms down as follows:

- It won't matter what order you do your operations in, you are guaranteed to get the same answer.  All of the axioms that deal with associativity, commutativity, compatibility, and distributivity are telling us this.
- Your vector space has an element that acts as a $\vec{0}$ element, and every vector has an opposite that, when you add them together, gives you $\vec{0}$.
- Your scalar field, which has a 1 in it, when combined with a vector, won't change that vector.  Or, $1 \cdot \vec{x} = \vec{x}$ 

But that doesn't answer why we say what we want in a vector space that way.  Even more, we don't have a good idea of what other structures my meet these axioms and be a vector space.  Or how to interpret that.


## Why are the axioms this way?

In elementary school, you learned your 1-digit by 1-digit times tables by rote.  And then you learned how you could leverage that to multiply numbers bigger than 1-digit together by using the 1-digit table entries, putting the results in certain columns, learning how to carry, write down partial sums, and add them together.  You need only know the 1-digit times tables and a procedure with special addition rules to handle the multiplication of any two numbers of any length.

*That* reduction to a small set of memorized multiplication facts and easy rules to combine the results to get us any arbitrary computation is what we are striving for in vector spaces.  We don't want to have to memorize the sum of every two-vector combination in the space, because by my last count, I'm pretty sure there are more than 57 vectors in $\mathbb{R}^2$ alone.  What we want is a nice way to get a fixed set of facts we have to know about, and find simple procedures to compute the facts only as we need them.

So here's the game plan, to try and tackle complex things like $\mathbb{R}^n$ and other things that will qualify as vector spaces.  We are going to reference some topics that will be familiar to you if you've dealt with linear algebras before, but haven't talked about yet here:


| Task | Vector space concept |
|-------------------------|----------------------|
| Get a fixed number of facts (vectors $\vec{x}_{i}$) we need to deal with, like the times table. | Pick linearly independent vectors $\vec{x}_{i}$.|
| Figure out how to express them as simple combinations of known facts| Linear combinations with scalars: $\vec{x} = \alpha_{1} \vec{x}_{1} + \alpha_{1} \vec{x}_{1} + ... + \alpha_{n} \vec{x}_{n}$. A minimal set of linearly independent vectors that can do this is called a *basis*. |
| Discover that for vector spaces (finite ones), any set of vectors that work to do this *always* has the same number of vectors | This is the dimensionality $N$ of the space.|
| Figure out how to calculate $\alpha_{i}$ for a given set of vectors. | To do this, we'll have to create the concept of the *inner product*.|
| Find out that lots of choices of $\vec{x}_{i}$ will work to express any vector, but *certain* ones are easier to compute the $\alpha_{i}$'s for. | These will be vectors that are orthogonal and of a certain size (magnitude = 1).  The word that captures both of these concepts is *orthonormal*. There are still lots of orthonormal sets.|
| Figure out how to take an arbirary linear independent set and construct an equivalent orthonormal set out of them, because they are easier to deal with. | This is the Gram-Schmidt theorem.|

This will be enough for now.  Once we get into functions *between* vector spaces, and pick out the special functions (the *linear* transforms), we'll talk about what *eigenvectors* and *eigenvalues* are.  But not yet.

The insight here is that for a given vector space, rather than work with the raw vectors themselves, it is easier to break them down into computations on known vectors (the basis), and do scalar multiplication (really, mappings that work like the underlying arithmetic on the scalars) on those basis vectors.  This is because the operations on the scalar field, and the way they map (or stretch) the basis vectors is ultimately an easier accounting scheme.  So we go through all of this work to find linear independent sets, bases, orthonormal bases, an inner product function, etc., so that we can use the inner product to easily get scalar coefficients and go on our merry way doing calculations on the vector space elements themselves in terms of their fewer and simpler basis elements.





















You could think about this as follows: all you have are the vectors, and you are given a really big set of rules that tells you what vector you get when you add any two vectors.  You just have to find that rule in the set.  It would be like memorizing your times tables way way far beyond your 1-digit by 1-digit table you have in elementary school.  Some savants do this to perform faster computations -- they may memorize all 2-digit by 2-digit products, and that can significantly speed up their already fast computations.  Some even have some key 3-digit by 3-digit products memorized.

If you think about it, in $\mathbb{R}^n$, that scalar relationship is a pattern you notice about how the vectors adhere to.  But it isn't necessary -- you could just memorize what resulting vector you get when you add any two vectors together.

Here's one of the guiding intuitions about vector spaces.  Vector spaces are huge.  Like, I'm pretty sure there are more than 57 vectors in $\mathbb{R}^2$ alone.





The point here is this: we have a good intuitive idea of how vectors behave in $\mathbb{R}$ spaces.  As we stated above, we can just follow our noses: scalars can be added and multiplied, vectors can be added, and we can put them together in ways such that if we construct a vector $\vec{x} = \alpha_{1} \vec{x_{1}} + \alpha_{2} \vec{x_{2}}$, we can figure out exactly what vector $\vec{x}$ we must have.

But we don't *have* to rely on that geometry of arrows in general, and how they stretch and add with the parralelogram rule.  We don't have to have any idea what the elements of the vector space really look like (as in, they don't have to be arrows in $\mathbb{R}$ space).  They just have to be objects that have a vector $+$ structure, and you can define some maps on those objects, which you can *label* with scalars (from $\mathbb{R}$ or $\mathbb{C}$, as we'll see), and they just have to obey the axioms.  We will need to lift off from this $\mathbb{R}$ prototype of vector spaces for some of the vector spaces we will use in quantum theory.  At first, you will want to use your intuition about $\mathbb{R}$ to reason about these more abstract vector spaces (the quantum "state space"), but it won't *quite* work, and you'll need this more sophisticated abstraction.

In the next chapter, we'll take some quantum phenomenon, and figure out how we pick linear algebra to model our phenomenon, and what tools from its toolbox we will need.  And illuminate a little more how linear algebra hangs together.
-->
